{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple MLP with no hidden layer and 10 output neurons with softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "import sys\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(55000, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#you could also get data from the web, this way\n",
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "#mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "dir=\"MNIST_data/\"\n",
    "# Import data\n",
    "mnist = input_data.read_data_sets(dir, one_hot=True)\n",
    "mnist.train.images.shape\n",
    "mnist.train.labels.shape\n",
    "mnist.test.images.shape\n",
    "mnist.test.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "def gen_image(arr):\n",
    "    two_d = (np.reshape(arr, (28, 28)) * 255).astype(np.uint8)\n",
    "    plt.imshow(two_d)\n",
    "    return plt\n",
    "\n",
    "gen_image(mnist.test.images[0]).show()\n",
    "gen_image(mnist.test.images[1]).show()\n",
    "print(mnist.test.labels[0], mnist.test.labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.matmul(x, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "for _ in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9183\n"
     ]
    }
   ],
   "source": [
    "# Test trained model\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images,\n",
    "                                  y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP with multiple hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import math\n",
    "\n",
    "dir=\"MNIST_data/\"\n",
    "NUM_CLASSES = 10\n",
    "# The MNIST images are always 28x28 pixels.\n",
    "IMAGE_SIZE = 28\n",
    "IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE\n",
    "batch_size=100\n",
    "hidden1=128\n",
    "hidden2=32\n",
    "learning_rate=0.01\n",
    "log_dir=\"logs/\"\n",
    "max_steps=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate placeholder variables to represent the input tensors.\n",
    "def placeholder_inputs(batch_size):\n",
    "  images_placeholder = tf.placeholder(tf.float32, shape=(batch_size, IMAGE_PIXELS))\n",
    "  labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "  return images_placeholder, labels_placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fills the feed_dict for training the given step with the next \"batch size\" examples\n",
    "def fill_feed_dict(data_set, images_pl, labels_pl):\n",
    "  images_feed, labels_feed = data_set.next_batch(batch_size)\n",
    "  feed_dict = {\n",
    "      images_pl: images_feed,\n",
    "      labels_pl: labels_feed,\n",
    "  }\n",
    "  return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Runs one evaluation against the full epoch of data.\n",
    "#do_eval(sess,eval_correct,images_placeholder,labels_placeholder,data_sets.train)\n",
    "def do_eval(sess,eval_correct,images_placeholder,labels_placeholder,data_set):\n",
    "  true_count = 0  # Counts the number of correct predictions.\n",
    "  steps_per_epoch = data_set.num_examples // batch_size\n",
    "  num_examples = steps_per_epoch * batch_size\n",
    "  for step in range(steps_per_epoch):\n",
    "    feed_dict = fill_feed_dict(data_set,images_placeholder,labels_placeholder)\n",
    "    true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "  precision = float(true_count) / num_examples\n",
    "  print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %\n",
    "        (num_examples, true_count, precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Build the MNIST model\n",
    "def inference(images, hidden1_units, hidden2_units):\n",
    "  # Hidden 1\n",
    "  with tf.name_scope('hidden1'):\n",
    "    weights = tf.Variable(tf.truncated_normal([IMAGE_PIXELS, hidden1_units],\n",
    "                            stddev=1.0 / math.sqrt(float(IMAGE_PIXELS))), name='weights')\n",
    "    biases = tf.Variable(tf.zeros([hidden1_units]), name='biases')\n",
    "    hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)\n",
    "  # Hidden 2\n",
    "  with tf.name_scope('hidden2'):\n",
    "    weights = tf.Variable(tf.truncated_normal([hidden1_units, hidden2_units],\n",
    "                            stddev=1.0 / math.sqrt(float(hidden1_units))), name='weights')\n",
    "    biases = tf.Variable(tf.zeros([hidden2_units]), name='biases')\n",
    "    hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "  # Linear\n",
    "  with tf.name_scope('softmax_linear'):\n",
    "    weights = tf.Variable(tf.truncated_normal([hidden2_units, NUM_CLASSES],\n",
    "                            stddev=1.0 / math.sqrt(float(hidden2_units))), name='weights')\n",
    "    biases = tf.Variable(tf.zeros([NUM_CLASSES]), name='biases')\n",
    "    logits = tf.matmul(hidden2, weights) + biases\n",
    "  return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculates the loss from the logits and the labels.\n",
    "def lossFn(logits, labels):\n",
    "  labels = tf.to_int64(labels)\n",
    "  cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits, name='xentropy')\n",
    "  return tf.reduce_mean(cross_entropy, name='xentropy_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(loss, learning_rate):\n",
    "  # Add a scalar summary for the snapshot loss.\n",
    "  tf.summary.scalar('loss', loss)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  # Create a variable to track the global step.\n",
    "  global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "  train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "  return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluate the quality of the logits at predicting the label.\n",
    "def evaluation(logits, labels):\n",
    "  # For a classifier model, we can use the in_top_k Op.\n",
    "  # It returns a bool tensor with shape [batch_size] that is true for\n",
    "  # the examples where the label is in the top k (here k=1)\n",
    "  # of all logits for that example.\n",
    "  correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "  # Return the number of true entries.\n",
    "  return tf.reduce_sum(tf.cast(correct, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train MNIST for a number of steps\n",
    "def run_training():\n",
    "  # Get the sets of images and labels for training, validation, and\n",
    "  # test on MNIST.\n",
    "  data_sets = input_data.read_data_sets(dir)\n",
    "  with tf.Graph().as_default():\n",
    "    images_placeholder, labels_placeholder = placeholder_inputs(batch_size)\n",
    "    # Build a Graph that computes predictions from the inference model.\n",
    "    logits = inference(images_placeholder, hidden1, hidden2)\n",
    "    loss = lossFn(logits, labels_placeholder)\n",
    "    train_op = training(loss, learning_rate)\n",
    "    eval_correct = evaluation(logits, labels_placeholder)\n",
    "    summary = tf.summary.merge_all()\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    sess = tf.Session()\n",
    "    summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "\n",
    "    sess.run(init)\n",
    "    for step in range(max_steps):\n",
    "      start_time = time.time()\n",
    "      feed_dict = fill_feed_dict(data_sets.train, images_placeholder, labels_placeholder)\n",
    "      #The return values are the activations\n",
    "      # from the `train_op` (which is discarded) and the `loss` Op.  To\n",
    "      # inspect the values of your Ops or variables, you may include them\n",
    "      # in the list passed to sess.run() and the value tensors will be\n",
    "      # returned in the tuple from the call.\n",
    "      _, loss_value = sess.run([train_op, loss],feed_dict=feed_dict)\n",
    "      duration = time.time() - start_time\n",
    "\n",
    "      # Write the summaries and print an overview fairly often.\n",
    "      if step % 100 == 0:\n",
    "        print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "        summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "        summary_writer.add_summary(summary_str, step)\n",
    "        summary_writer.flush()\n",
    "\n",
    "      # Save a checkpoint and evaluate the model periodically.\n",
    "      if (step + 1) % 1000 == 0 or (step + 1) == max_steps:\n",
    "        checkpoint_file = os.path.join(log_dir, 'model.ckpt')\n",
    "        saver.save(sess, checkpoint_file, global_step=step)\n",
    "        # Evaluate against the training set.\n",
    "        print('Training Data Eval:')\n",
    "        do_eval(sess,eval_correct,images_placeholder,labels_placeholder,data_sets.train)\n",
    "        # Evaluate against the validation set.\n",
    "        print('Validation Data Eval:')\n",
    "        do_eval(sess,eval_correct,images_placeholder,labels_placeholder,data_sets.validation)\n",
    "        # Evaluate against the test set.\n",
    "        print('Test Data Eval:')\n",
    "        do_eval(sess,eval_correct,images_placeholder,labels_placeholder,data_sets.test)\n",
    "    summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Step 0: loss = 2.32 (0.550 sec)\n",
      "Step 100: loss = 2.22 (0.005 sec)\n",
      "Step 200: loss = 2.00 (0.003 sec)\n",
      "Step 300: loss = 1.76 (0.006 sec)\n",
      "Step 400: loss = 1.47 (0.004 sec)\n",
      "Step 500: loss = 1.11 (0.005 sec)\n",
      "Step 600: loss = 0.88 (0.003 sec)\n",
      "Step 700: loss = 0.76 (0.003 sec)\n",
      "Step 800: loss = 0.68 (0.005 sec)\n",
      "Step 900: loss = 0.48 (0.005 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 47488  Precision @ 1: 0.8634\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4356  Precision @ 1: 0.8712\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 8690  Precision @ 1: 0.8690\n",
      "Step 1000: loss = 0.51 (0.031 sec)\n",
      "Step 1100: loss = 0.51 (0.320 sec)\n",
      "Step 1200: loss = 0.69 (0.006 sec)\n",
      "Step 1300: loss = 0.37 (0.009 sec)\n",
      "Step 1400: loss = 0.50 (0.007 sec)\n",
      "Step 1500: loss = 0.52 (0.007 sec)\n",
      "Step 1600: loss = 0.34 (0.004 sec)\n",
      "Step 1700: loss = 0.43 (0.005 sec)\n",
      "Step 1800: loss = 0.29 (0.003 sec)\n",
      "Step 1900: loss = 0.50 (0.003 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 49275  Precision @ 1: 0.8959\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4517  Precision @ 1: 0.9034\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9018  Precision @ 1: 0.9018\n",
      "Step 2000: loss = 0.35 (0.031 sec)\n",
      "Step 2100: loss = 0.21 (0.003 sec)\n",
      "Step 2200: loss = 0.47 (0.557 sec)\n",
      "Step 2300: loss = 0.36 (0.005 sec)\n",
      "Step 2400: loss = 0.21 (0.004 sec)\n",
      "Step 2500: loss = 0.25 (0.004 sec)\n",
      "Step 2600: loss = 0.36 (0.003 sec)\n",
      "Step 2700: loss = 0.38 (0.004 sec)\n",
      "Step 2800: loss = 0.23 (0.032 sec)\n",
      "Step 2900: loss = 0.31 (0.003 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 49966  Precision @ 1: 0.9085\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4587  Precision @ 1: 0.9174\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9119  Precision @ 1: 0.9119\n",
      "Step 3000: loss = 0.36 (0.054 sec)\n",
      "Step 3100: loss = 0.19 (0.003 sec)\n",
      "Step 3200: loss = 0.46 (0.004 sec)\n",
      "Step 3300: loss = 0.33 (0.422 sec)\n",
      "Step 3400: loss = 0.31 (0.013 sec)\n",
      "Step 3500: loss = 0.38 (0.003 sec)\n",
      "Step 3600: loss = 0.35 (0.005 sec)\n",
      "Step 3700: loss = 0.33 (0.004 sec)\n",
      "Step 3800: loss = 0.33 (0.004 sec)\n",
      "Step 3900: loss = 0.23 (0.003 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 50351  Precision @ 1: 0.9155\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4616  Precision @ 1: 0.9232\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9190  Precision @ 1: 0.9190\n",
      "Step 4000: loss = 0.26 (0.046 sec)\n",
      "Step 4100: loss = 0.22 (0.003 sec)\n",
      "Step 4200: loss = 0.44 (0.003 sec)\n",
      "Step 4300: loss = 0.36 (0.003 sec)\n",
      "Step 4400: loss = 0.39 (0.293 sec)\n",
      "Step 4500: loss = 0.15 (0.002 sec)\n",
      "Step 4600: loss = 0.47 (0.003 sec)\n",
      "Step 4700: loss = 0.22 (0.004 sec)\n",
      "Step 4800: loss = 0.30 (0.004 sec)\n",
      "Step 4900: loss = 0.21 (0.002 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 50782  Precision @ 1: 0.9233\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4638  Precision @ 1: 0.9276\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9250  Precision @ 1: 0.9250\n"
     ]
    }
   ],
   "source": [
    "import shutil,os\n",
    "#if os.path.exists(log_dir):\n",
    "#    shutil.rmtree(log_dir)\n",
    "tf.gfile.MakeDirs(log_dir)\n",
    "run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
